{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kaggle进行情感分析\n",
    "### 数据集 https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
    "### 子豪兄NLP视频 https://www.bilibili.com/video/av57066293\n",
    "- 不要恐惧kaggle\n",
    "- 不管黑猫白猫，抓到猫就是好猫。算法简单有效就行\n",
    "- 数据处理与特征工程很重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train.tsv',sep='\\t') #tsv的读取采用分隔符为\\t\n",
    "data_test = pd.read_csv('test.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立语料库\n",
    "- 将句子看作是向量，文本的特征工程\n",
    "- 常用的词袋模型 TF-IDF模型 word2vec模型\n",
    "- 语料库，训练和测试集所有文本内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = data_train['Phrase']\n",
    "test_sentences = data_test['Phrase']\n",
    "sentences = pd.concat([train_sentences,test_sentences])\n",
    "\n",
    "label = data_train['Sentiment']\n",
    "stop_words = open('stop_words.txt',encoding='utf-8').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选取 - 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=150000, min_df=1,\n",
       "        ngram_range=(1, 4), preprocessor=None,\n",
       "        stop_words=[\"\\ufeffain'\", 'happy', 'isn', 'ain', 'al', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'sn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'ZT', 'ZZ', 'a', \"a's\", 'able', 'about', 'above', 'abst', 'accordance', 'accor...', ',', '·', '￥', '……', '（', '）', '——', '、', '：', '；', '“', '’', '《', '》', '，', '。', '、', '？', '★ '],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用sklearn库中的CountVectorizer构建词袋模型\n",
    "# 词袋模型的详细介绍请看子豪兄的视频\n",
    "# analyzer='word'指的是以词为单位进行分析，对于拉丁语系语言，有时需要以字母'character'为单位进行分析\n",
    "# ngram指分析相邻的几个词，避免原始的词袋模型中词序丢失的问题\n",
    "# max_features指最终的词袋矩阵里面包含语料库中出现次数最多的多少个词\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "co = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1,4), #1个词-4个词 都提取出来\n",
    "    stop_words=stop_words,\n",
    "    max_features=150000\n",
    ")\n",
    "# 使用语料库，构建词袋模型\n",
    "co.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集随机拆分为新的训练集和验证集，默认3:1,然后进行词频统计\n",
    "# 新的训练集和验证集都来自于最初的训练集，都是有标签的。\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(train_sentences, label, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用上面构建的词袋模型，把训练集和验证集中的每一个词都进行特征工程，变成向量\n",
    "x_train = co.transform(x_train)\n",
    "x_test = co.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x150000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0] #稀疏矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression()\n",
    "lg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词袋模型进行特征选取，分类器为sklearn默认的logistic分类器\n",
      "train acc: 0.7685322495011077\n",
      "valid acc: 0.6412277329232347\n"
     ]
    }
   ],
   "source": [
    "print('词袋模型进行特征选取，分类器为sklearn默认的logistic分类器')\n",
    "\n",
    "print('train acc:' ,lg.score(x_train,y_train))\n",
    "print('valid acc:' ,lg.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "\n",
       "   Sentiment  \n",
       "0          3  \n",
       "1          3  \n",
       "2          2  \n",
       "3          3  \n",
       "4          3  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试集预测结果\n",
    "test_X = co.transform(test_sentences) #词袋模型转为向量\n",
    "prediction = lg.predict(test_X) \n",
    "\n",
    "data_test.loc[:,'Sentiment'] = prediction\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存结果\n",
    "final_data = data_test.loc[:,['PhraseId','Sentiment']]\n",
    "final_data.to_csv('final_data.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素bayes分类器 速度更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.7125098405375222\n",
      "valid acc: 0.6088256653423897\n",
      "词袋方法进行文本特征工程，使用sklearn默认的多项式朴素贝叶斯分类器，验证集上的预测准确率: 0.6088256653423897\n"
     ]
    }
   ],
   "source": [
    "#引用朴素贝叶斯进行分类训练和预测\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train,y_train)\n",
    "print('词袋模型进行特征选取，分类器为sklearn默认的bayes分类器')\n",
    "print('train acc:' ,classifier.score(x_train,y_train))\n",
    "print('valid acc:' ,classifier.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选取 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF值衡量了一个词出现的次数，即词频(Term Frequency)。<br>\n",
    "IDF值衡量了这个词是不是烂大街。逆文本频率指数（Inverse Document Frequency）如果是the、an、a等烂大街的词，IDF值就会很低。<br>\n",
    "例如，“中国”、“功夫”这两个词也许会同时出现，但“中国”这个词在各个文档中都有出现，IDF值很低，因此TF_IDF值也很低。 而“功夫”这个词只在特定文档中出现，这个词能带来的“特异性”信息就会大很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "TF(“功夫\")&=\\frac{\"功夫\"这个词在当前文章中出现的次数}{\"功夫\"这个词在整个语料库中出现的次数}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "IDF(“功夫\")&=ln \\frac{语料库的总文档数}{语料库中\"功夫\"出现的文档数}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "TF\\_IDF(“功夫\")&=IF(“功夫\") \\times IDF(“功夫\")\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用sklearn库中的TfidfVectorizer构建TF-IDF模型\n",
    "# TF-IDF模型的详细介绍请看子豪兄的视频\n",
    "# analyzer='word'指的是以词为单位进行分析，对于拉丁语系语言，有时需要以字母'character'为单位进行分析\n",
    "# ngram指分析相邻的几个词，避免原始的词袋模型中词序丢失的问题\n",
    "# max_features指最终的词袋矩阵里面包含语料库中出现次数最多的多少个词\n",
    "\n",
    "# TF-IDF模型是专门用来过滤掉烂大街的词的，所以不需要引入停用词stop_words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1,4),\n",
    "    # stop_words=stop_words,\n",
    "    max_features=150000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=150000, min_df=1,\n",
       "        ngram_range=(1, 4), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和验证集，并转为tf-idf特征\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(train_sentences,label,random_state=1234)\n",
    "\n",
    "x_train = tf_idf.transform(x_train)\n",
    "x_test = tf_idf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型进行特征选取，分类器为sklearn默认的logistic分类器\n",
      "train acc: 0.6708872655816139\n",
      "valid acc: 0.6045367166474432\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train,y_train)\n",
    "print('TF-IDF模型进行特征选取，分类器为sklearn默认的bayes分类器')\n",
    "print('train acc:' ,classifier.score(x_train,y_train))\n",
    "print('valid acc:' ,classifier.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型进行特征选取，分类器为sklearn默认的logistic分类器\n",
      "train acc: 0.7058481780511769\n",
      "test acc: 0.6326541073945918\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(x_train,y_train)\n",
    "print('TF-IDF模型进行特征选取，分类器为sklearn默认的logistic分类器')\n",
    "print('train acc:' ,lg.score(x_train,y_train))\n",
    "print('test acc:' ,lg.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic模型的优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型进行特征选取，分类器为sklearn默认的logistic分类器,优化后的结果\n",
      "train acc: 0.7719595027553505\n",
      "test acc: 0.6533384595668332\n"
     ]
    }
   ],
   "source": [
    "# C：正则化系数，C越小，正则化效果越强\n",
    "# dual：求解原问题的对偶问题\n",
    "lg2 = LogisticRegression(C=3,dual=True)\n",
    "lg2.fit(x_train,y_train)\n",
    "print('TF-IDF模型进行特征选取，分类器为sklearn默认的logistic分类器,优化后的结果')\n",
    "print('train acc:' ,lg2.score(x_train,y_train))\n",
    "print('test acc:' ,lg2.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic模型的参数寻优\n",
    "针对C，dual可以进行网格式搜索，搜索空间：C从1到9。对每一个C，都分别尝试dual为True和False的两种参数。<br>\n",
    "最后从所有参数中挑出能够使模型在验证集上预测准确率最高的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': range(1, 10), 'dual': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C':range(1,10),\n",
    "              'dual':[True,False]\n",
    "             }\n",
    "lg3 = LogisticRegression()\n",
    "grid = GridSearchCV(lg3,param_grid=param_grid)\n",
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 5, 'dual': True}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_final = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网格搜索最优参数后，正确率：\n",
      "train acc: 0.7973172711350336\n",
      "test acc: 0.6546456491093169\n"
     ]
    }
   ],
   "source": [
    "print('网格搜索最优参数后，正确率：')\n",
    "print('train acc:' ,lg_final.score(x_train,y_train))\n",
    "print('test acc:' ,lg_final.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
